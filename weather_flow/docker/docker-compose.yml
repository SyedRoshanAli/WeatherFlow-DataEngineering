name: ds463-final-project

services:
  # Zookeeper service (required for Kafka)
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2182:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - weather-network

  # Kafka message broker
  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9094:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "weather-raw:1:1,weather-processed:1:1"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_MESSAGE_MAX_BYTES: "20971520"
      KAFKA_MAX_REQUEST_SIZE: "20971520"
      KAFKA_SOCKET_REQUEST_MAX_BYTES: "20971520"
      KAFKA_REPLICA_FETCH_MAX_BYTES: "20971520"
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms512m"
      KAFKA_JVM_PERFORMANCE_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35"
    restart: on-failure:3
    networks:
      - weather-network

  # Spark master node
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8080"  # Web UI
      - "7078:7077"  # Spark master port
    networks:
      - weather-network

  # Spark worker node
  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    networks:
      - weather-network

  # Airflow webserver and scheduler
  airflow:
    build:
      context: ..
      dockerfile: docker/Dockerfile-airflow
    container_name: airflow
    depends_on:
      - kafka
      - spark-master
      - postgres
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/plugins:/opt/airflow/plugins
      - ../data:/opt/airflow/data:ro
      - ../scripts:/opt/airflow/weather_scripts:ro
      - /tmp:/tmp
    ports:
      - "8091:8080"  # Airflow webserver UI
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__WEB_SERVER_HOST=0.0.0.0
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8091
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__WEBSERVER__ACCESS_LOGFILE=-
      - AIRFLOW__WEBSERVER__ERROR_LOGFILE=-
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - PYTHONPATH=/opt/airflow
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=60
      - AIRFLOW__CORE__MIN_FILE_PROCESS_INTERVAL=5
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
      - AIRFLOW__CORE__PARALLELISM=1
      - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=1
      - AIRFLOW__SCHEDULER__MAX_THREADS=1
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=/opt/airflow/logs/scheduler
      - AIRFLOW__CORE__IGNORE_SUBDIR_IN_DAGS_FOLDER=scripts
    command: bash -c "airflow db migrate && (airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true) && sleep 10 && airflow webserver -p 8080 & airflow scheduler"
    networks:
      - weather-network
  
  # Weather data producer service
  weather-producer:
    build:
      context: ..
      dockerfile: docker/Dockerfile-kafka
    container_name: weather-producer
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - OPENWEATHER_API_KEY=${OPENWEATHER_API_KEY}
      - WEATHERAPI_KEY=1f5800c837ab40bfbe2161243251305
      - PYTHONPATH=/app
    command: bash -c "sleep 30 && cd /app && python -u /app/kafka/scripts/simple_producer.py"
    volumes:
      - ../data:/app/data
      - ../kafka/scripts:/app/kafka/scripts
    restart: on-failure:3
    networks:
      - weather-network

  # Weather data consumer service
  weather-consumer:
    build:
      context: ..
      dockerfile: docker/Dockerfile-kafka
    container_name: weather-consumer
    depends_on:
      - kafka
      - weather-producer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - PYTHONPATH=/app
    command: bash -c "sleep 45 && cd /app && python -u /app/kafka/scripts/simple_consumer.py"
    volumes:
      - ../data:/app/data
      - ../kafka/scripts:/app/kafka/scripts
    restart: on-failure:3
    networks:
      - weather-network

  # Kafka UI (Kafdrop)
  kafka-ui:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
      - JVM_OPTS=-Xms256M -Xmx512M
      - SERVER_SERVLET_CONTEXTPATH=/
    restart: on-failure:3
    networks:
      - weather-network

  # Dashboard application
  dashboard:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: dashboard
    depends_on:
      - weather-consumer
    ports:
      - "8051:8051"  # Dashboard UI
    environment:
      - DASHBOARD_PORT=8051
      - PYTHONUNBUFFERED=1
      - FILE_ACCESS_TIMEOUT=10
      - FILE_ACCESS_RETRIES=5
      - PYTHONPATH=/app
    command: bash -c "cd /app && python -m dashboard.app"
    volumes:
      - ../data:/app/data:ro
      - /tmp:/tmp
    networks:
      - weather-network
    restart: unless-stopped

  # PostgreSQL for Airflow
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    ports:
      - "5432:5432"
    networks:
      - weather-network

# Define network
networks:
  weather-network:
    driver: bridge

# Define volumes
volumes:
  postgres-db-volume:
  weather-data:
